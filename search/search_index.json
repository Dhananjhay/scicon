{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Official CANFAR Science Platform Documentation","text":""},{"location":"#introduction-and-access","title":"Introduction and Access","text":"<p>The CANFAR Science Platform consists of set of services and resources to enable cloud-based astronomy data analysis.  Browser-based access to CANFAR's cloud computing layer is provided via authorized access to the CANFAR Portal. A Canadian Astronomy Data Centre (CADC) account that has been authorized to use the Portal is required.</p> <ul> <li>To request a CADC Account:  https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/auth/request.html</li> <li> <p>Authorization to access the CANFAR Portal:</p> <ul> <li>If the project you are using the resource for is part of a collaboration already using the CANFAR Science Portal, ask the administrator of the collaboration you belong to add you as a member of the collaborations access group using the CADC Group Management Interface</li> <li>If your project is not part of an authorized collaboration, the collaboration lead will need to request access by sending an email to support@canfar.net specifying they are requesting access to the CANFAR Science Portal, the scale of resources needed (storage and cores of compute) and a short (few line) resource justification.  Canadian research collaborations are encouraged to apply for CANFAR Science Portal authorization.</li> </ul> </li> </ul> <p>The CANFAR Science Portal runs software packaged in containers. The portal allows users to run both pre-built, shared containers or private, custom containers. Authorized collaboration members can publish container images to the CANFAR Container Images Registry.  We have specific documentation on how to build and publish containers capable of being launched within the CANFAR Science Portal.</p> <p>The CANFAR Science Platform supports both launching interactive sessions (via the Portal) and non-interactive ones (using cURL or a dedicated Python module). More detailed documentation on launching a computing session on the CANFAR Science Portal can be found here. </p>"},{"location":"#interactive-sessions","title":"Interactive Sessions","text":"<p>Interactive sessions are applications running on the CANFAR cloud infrastructure and are accessed via a web browser, allowing users to interact with the (typically large) datasets hosted on CANFAR Science Platform storage. There are a few types of Interactive Sessions that cab be launched through the portal:</p>"},{"location":"#notebooks","title":"Notebooks","text":"<p>Notebooks are using the Jupyter Lab interface.</p>"},{"location":"#carta","title":"CARTA","text":"<p>CARTA (Cube Analysis and Rendering Tool for Astronomy) is an astronomy visualization tool that will run natively in the browser. It can read FITS or HDF5 files, often used in radio astronomy, but not only.</p>"},{"location":"#desktop","title":"Desktop","text":""},{"location":"#an-x11-desktop-session-that-enables-running-applications-in-the-science-platform-a-browser-desktop-session-can-be-launched","title":"An X11-desktop session that enables running applications in the Science Platform, a browser Desktop session can be launched.","text":"<ul> <li>Desktop documentation and tutorials are described in more detail in the User Documentation</li> <li>Launching a CASA window in the Desktop YouTube tutorial:  YouTube Tutorial</li> </ul>"},{"location":"#contributed","title":"Contributed","text":"<p>Contributed sessions are user-customised web applications, typically not maintained by CANFAR. This can be anything, such as a VSCode server and Pluto notebook for the Julia language. </p> <p>Please refer to the container documentation for more information on building contributed sessions.</p>"},{"location":"#batch-jobs","title":"Batch Jobs","text":"<p>Currently, the CANFAR Science Platform has a limited capacity for batch processing.  Batch processing can be understood  as a non-interactive executable launched on a container whose output is not attached to a display (headless). Please contact support@canfar.net before making use of the headless job support -- we are incrementally adding support for batch processing in the science platform. See the specific documentation. This is still experimental and the API may change.</p>"},{"location":"#storage","title":"Storage","text":"<p>All sessions and applications accessed through the Science Platform (interactive and batch) share a common storage system in the directory <code>/arc</code>. The primary folder/directories <code>/arc/home</code> and <code>/arc/projects</code>.  <code>/arc/home/${USERNAME}</code> contains a user's environment initializations details and personal information that might not be shared with collaboration members. <code>/arc/projects/${GROUP_NAME}</code> holds the data that the given group will be processing and the outputs of that processing. CANFAR encourages the use of <code>/arc/projects</code> for most data, and <code>/arc/home</code> for personalized configuration and software.  By default a user's <code>/arc/home/${USER}</code> folder is not readable by others on the platform.</p> <p>An efficient and convenient way to access the <code>arc</code> storage outside the Science Platform is through <code>sshfs</code>. Here is the documentation.</p> <p>In addition to <code>sshfs</code> mentioned above, the <code>arc</code> storage is also accessible via an API that is exposed:</p> <ul> <li>Using the CANFAR storage management interface: https://www.canfar.net/storage/arc/list</li> <li>Using the VOSpace Python libraries</li> <li>Using the <code>/arc/files</code> URL endpoint documentation</li> </ul> <p>More detailed instructions on data transfer options are documented here: here</p> <p>Please take care to protect sensitive information by ensuring it is not publicly accessible.  File access control on <code>arc</code> is described in the next section.</p>"},{"location":"#groups-and-permissions","title":"Groups and Permissions","text":"<p>Projects are encouraged to use groups to manage access to resources, including files and directories/folders in the <code>arc</code> project storage, mounted at <code>/arc/projects</code> in every session.</p> <p>Groups and their memberships can be managed through the CANFAR groups web interface, here: https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/groups/</p> <p>Once created, groups can be assigned to files and directories in arc storage directly from their interactive sessions, or through the CANFAR storage</p> <p>For more details on setting access permissions, see the documentation on file permissions</p>"},{"location":"#programmatic-access","title":"Programmatic Access","text":"<p>Session launching and management are through the <code>skaha</code> service. The <code>skaha</code> API definition and science platform service are here:  https://ws-uv.canfar.net/skaha</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Discussions of issues and platform features take place in the Science Platform Slack channel: Science Platform Slack Channel</p> <p>To report bugs and request new features, please use our GitHub pages:  - For the infrastructure and session, https://github.com/opencadc/science-platform/issues - For the containers: https://github.com/opencadc/science-containers/issues</p> <p>Contributions to the platform (including updates or corrections to the documentation) can be submitted as pull requests to this GitHub repository. We especially encourage science containers to be shared across the user community by making your published containers public.</p> <p>General inquiries can be made to support@canfar.net, and take a look at our FAQ.</p> <p></p>"},{"location":"faq/","title":"CANFAR Science Platform FAQ","text":"<ul> <li>My session is stuck in the <code>Pending</code> state - This can imply that the platform is unable to launch your image.  There are a number of potential causes:</li> <li> <p>Often skaha fails to authorize you to https://images.canfar.net due to an expired <code>CLI Secret</code>.  Try resetting this value by logging into https://images.canfar.net (using the OIDC Login button), going to your User Profile, and updating your CLI Secret.  Once done you should delete the Pending session and try launching it again.</p> <ul> <li>If the image is proprietary and the CLI Secret update did not work, check with your project administrator to ensure you have been granted access to the project in https://images.canfar.net</li> <li>The session could be in a Pending state waiting for resources so that it can be scheduled.</li> <li>More information about the reason for the Pending state can be found using the logging mechanisms explained in Programmatic Access.</li> </ul> </li> <li> <p>How do I test a graphical container on my Mac?</p> </li> <li> <p>Enable \"Allow connections from network clients\" in XQuartz settings. Relaunch XQuartz.</p> </li> <li> <p>In terminal execute</p> <pre><code>&gt; xhost + 127.0.0.1\n</code></pre> </li> <li> <p>Launch docker run with the option</p> <pre><code>-e DISPLAY=host.docker.internal:0\n</code></pre> </li> </ul> <p>These steps were taken from https://medium.com/@mreichelt/how-to-show-x11-windows-within-docker-on-mac-50759f4b65cb</p> <p></p>"},{"location":"headless/","title":"CANFAR Science Platform Headless Jobs","text":"<p>Please contact us before making use of the 'headless job' support--we are incrementally adding support for batch processing in the science platform.</p>"},{"location":"headless/#create-an-image","title":"Create an image","text":"<p>Create an image as per the regular process of making containers available in the platform:  Publishing</p> <p>However, label it as <code>headless</code> in https://images.canfar.net to make it available for headless job launching.</p>"},{"location":"headless/#launch-a-headless-job","title":"Launch a headless job","text":"<p>For the full details of the job launching API, see this section of the akaha API documentation:  https://ws-uv.canfar.net/skaha#!/Session_Management/post_session</p> <p>All jobs will be run as the calling user.  All jobs have the <code>/arc</code> filesystem mounted.</p> <p>Example: launch a headless job, overriding the command and providing two arguments:</p> <p><code>curl -E ~/.ssl/cadcproxy.pem https://ws-uv.canfar.net/skaha/v0/session -d \"name=headless-test\" -d \"image=images.canfar.net/skaha/terminal:0.1\" --data-urlencode \"cmd=touch\" --data-urlencode \"args=/arc/home/majorb/headless-test-1a /arc/home/majorb/headless-test-1b\"</code></p> <p>skaha will return the <code>sessionID</code> on a successful post (job launch).  The job will remain in the system for 1 hour after completion (success or failure).</p> <p>Job phases: - Pending - Running - Succeeded - Failed - Terminating - Unknown</p> <p>To view all sessions and jobs: <code>curl -E ~/.ssl/cadcproxy.pem https://ws-uv.canfar.net/skaha/v0/session</code></p> <p>To view a single session or job: <code>curl -E ~/.ssl/cadcproxy.pem https://ws-uv.canfar.net/skaha/v0/session/&lt;sessionID&gt;</code></p> <p>To view logs for session: <code>curl -E ~/.ssl/cadcproxy.pem https://ws-uv.canfar.net/skaha/v0/session/&lt;sessionID&gt;?view=logs</code></p> <p>This shows the complete output (stdout and stderr) for the image for the job.</p> <p>To view scheduling events for session: <code>curl -E ~/.ssl/cadcproxy.pem https://ws-uv.canfar.net/skaha/v0/session/&lt;sessionID&gt;?view=events</code></p> <p>Scheduling events will only be seen when there are issues scheduling the job on a node.</p>"},{"location":"headless/#community-and-support","title":"Community and Support","text":"<p>Dicussions of issues and platform features take place in the Science Platform Slack Channel:  Science Platform Slack Channel</p> <p>Reporting of bugs and new feature requests can also be made as github issues:  https://github.com/opencadc/skaha/issues</p> <p>Contributions to the platform (including updates or corrections to the documentation) can be submitted as pull requests to this GitHub repository.</p> <p>General inquiries can be made to support@canfar.net</p>"},{"location":"headless/#faq","title":"FAQ","text":"<ul> <li>My session is stuck in the <code>Pending</code> state - This can imply that the platform is unable to launch your image.  There are a number of potential causes:</li> <li> <p>Often skaha fails to authorize you to https://images.canfar.net due to an expired <code>CLI Secret</code>.  Try resetting this value by logging into https://images.canfar.net (using the OIDC Login button), going to your User Profile, and updating your CLI Secret.  Once done you should delete the Pending session and try launching it again.</p> <ul> <li>If the image is proprietary and the CLI Secret update did not work, check with your project administrator to ensure you have been granted access to the project in https://images.canfar.net</li> <li>The session could be in a Pending state waiting for resources so that it can be scheduled.</li> <li>More information about the reason for the Pending state can be found using the logging mechanisms explained in Programmatic Access.</li> </ul> </li> <li> <p>How do I test a graphical container on my Mac?</p> </li> <li>See the instructions to have container display shown on your Mac here:  Display ENV on OSX</li> </ul> <p></p>"},{"location":"permissions/","title":"CANFAR Authorisations and Permissions","text":""},{"location":"permissions/#seesion-authorisations","title":"Seesion Authorisations","text":"<p>The session launching and management is through the <code>skaha</code> service The skaha API definition and science platform service are here:  https://ws-uv.canfar.net/skaha</p>"},{"location":"permissions/#authentication","title":"Authentication","text":"<p>All requests to the skaha API must be made with CADC credentials.  In the science platform the credentials are handled with cookies, but for programatic access, either x.509 client certificates or authorization tokens must be used.</p>"},{"location":"permissions/#authorization-tokens","title":"Authorization Tokens","text":"<p>Tokens can be obtained from the CANFAR Access Control service by providing your CADC username and password over a secure SSL connection:</p> <p><code>curl https://ws-cadc.canfar.net/ac/login -d \"username=&lt;username&gt;\" -d \"password=&lt;password&gt;\"</code></p> <p>The token returned can then be used for making authenticated requests to skaha.  For example:</p> <p><code>curl -H \"Authorization: Bearer &lt;token&gt;\" https://ws-uv.canfar.net/skaha/v0/session</code></p> <p>Tokens are valid for 48 hours.</p>"},{"location":"permissions/#proxy-certificates","title":"Proxy Certificates","text":"<p>Another way to authenticate to the skaha API is by using proxy certificates.  Using the CADC Python libraries, the <code>cadc-get-cert</code> tool will download a proxy certificate to the default location: <code>$HOME/.ssl/cadcproxy.pem</code>.</p> <p><code>cadc-get-cert -u &lt;username&gt;</code></p> <p>By default the proxy certificate is valid for 10 days.  This can be modified (to a maximum of 30 days) with the <code>--days-valid</code> parameter.</p> <p>Instead of prompting for your password, cadc-get-cert can read it from your <code>$HOME/.netrc</code> file using the <code>--netrc-file</code> parameter.</p>"},{"location":"permissions/#canfar-arc-file-system-groups-and-permissions","title":"CANFAR <code>arc</code> File System Groups and Permissions","text":"<p>Groups can be assigned as either <code>read-only</code> or <code>read-write</code>.</p> <p>More sophisticated management of groups, including setting default groups for a given project directory, can be done on the command line in the science portal, and is explained in the section below.</p>"},{"location":"permissions/#command-line-group-management","title":"Command Line Group Management","text":"<p>Each file or directory can have any of read \u00ae, write (w), or execute (x) permission.  For example, a file with read-write permission is describe with rw-.</p> <pre><code>r = read - can see the file or directory\nw = write - can modify the file or directory\nx = execute - for directories, means list children.  for files, means execute file\n- = does not have the given permission (r, w, or x, depending on the position of the -)\n</code></pre> <p>The following lists permission combinations for arc as seen on the command line:</p> <pre><code>read-only file permissions: r--\nread-write file permissions: rw-\nread-only directory permissions: r-x\nread-write directory permissions: rwx\n</code></pre> <p>Group permissions are stored in POSIX Access Control Lists (ACLs).  To view the group permissions on a given file or directory, run the following command:</p> <pre><code>getfacl file-or-directory\n</code></pre> <p>There are two relevant entries in the output:</p> <p>The named-group permissions, in the format <code>group:{group-name}:{permissions}</code>.  For example: <code>group:skaha-users:rw-</code></p> <p>Secondly, if a <code>mask</code> entry exists, it will change the actual (or effictive) permissions the group receives.  For example, if the following mask entry <code>mask::r-x</code> were applied to <code>group:skaha-users:rw-</code>, the effective permissions become <code>group:skaha-users:r--</code>  Effective permissions are calculated by doing an AND operation on each of the three correspsonding permissions (rwx).  The permission must exist in both the original group permissions and the mask for them to become effective.  If a mask entry does not exist, the group permissions are used directly.</p> <p>To make files and directories (and their children) inherit group permissions, run one of the following commands:</p> <p>Set the default read group: <pre><code>setfacl -d -m group:{group-name}:r-x {read-only-dir}\n</code></pre></p> <p>Set the default read-write group: <pre><code>setfacl -d -m group:{group-name}:rwx {read-write-dir}\n</code></pre></p> <p>The group permissions are not set on target directories themselves, only on newly created children. To set group permissions on a single file or directory, run one of the following commands:</p> <p>Set the read group: <pre><code>setfacl -m group:{group-name}:r-x {read-only-dir}\n</code></pre></p> <p>Set the read-write group: <pre><code>setfacl -m group:{group-name}:rwx {read-write-dir}\n</code></pre></p> <p>To set group permissions on an existing directory tree recursively, run one of the following commands:</p> <p>Set the read group: <pre><code>setfacl -R -m group:{group-name}:r-x {read-only-dir}\n</code></pre></p> <p>Set the read-write group: <pre><code>setfacl -R -m group:{group-name}:rwx {read-write-dir}\n</code></pre></p> <p>To set group permissions on an existing directory tree recursively, and to have new children in directories of that tree inherit the group permissions, run one of the following commands:</p> <p>Set the read group: <pre><code>setfacl -R -d -m group:{group-name}:r-x {read-only-dir}\n</code></pre></p> <p>Set the read-write group: <pre><code>setfacl -R -d -m group:{group-name}:rwx {read-write-dir}\n</code></pre></p> <p></p>"},{"location":"publishing/","title":"CANFAR Science Platform Containers","text":""},{"location":"publishing/#introduction","title":"Introduction","text":"<p>The CANFAR Science Platform supports various types of containers: <code>session</code>, <code>software</code>,  and <code>legacy desktop application</code></p> <ul> <li><code>Session</code> are containers launched as native browser interactive applications (i.e. HTML5/Websocket).</li> <li><code>Software</code> are containers launched with any kind of executable, installed with custom software stack. </li> <li><code>Legacy desktop application</code> are containers launched and viewed specifically through a desktop <code>session</code>.  </li> </ul>"},{"location":"publishing/#building-canfar-science-platform-containers","title":"Building CANFAR Science Platform Containers","text":""},{"location":"publishing/#minimum-requirements","title":"Minimum requirements","text":"<ul> <li>Containers must be based on a standard Linux x86_84 distribution.</li> <li>Containers must contain an SSSD client and have ACL capabilities if one want to interact with the <code>arc</code> storage</li> </ul>"},{"location":"publishing/#sssd-and-acl","title":"SSSD and ACL","text":"<p>For linux group id (<code>gid</code>) names to be resolved, the container must have an SSSD client and ACL tools installed, and must provide an <code>nsswitch.conf</code> file as described below.  If any of these are missing, only group IDs will be displayed (when <code>id</code> is typed for example), but file system authorization will continue to work as expected. The packages to install on a Debian-based Linux distribution are <code>sssd-client</code> and  <code>acl</code>.</p> <p>The file <code>/etc/nsswitch.conf</code> must include the <code>sss</code> module in the <code>passwd</code>, <code>shadow</code>, and <code>group</code> entries.  For example:</p> <pre><code>passwd:     sss files\nshadow:     files sss\ngroup:      sss files\n</code></pre>"},{"location":"publishing/#additional-requirements-for-legacy-desktop-application-containers","title":"Additional requirements for legacy desktop application containers","text":"<p>Examples of legacy desktop software containers are astronomy GUIs such as CASA, Topcat, Aladin, and customized containers such as Gemini processing containers which require desktop interaction.  Some of the recipes (Dockerfiles) for building these containers can be found in the desktop directory.  They can also be managed and hosted elsewhere. However, wherever the source is hosted, containers must meet a minimal set of requirements and expectations for execution in skaha. Also the default executuable is <code>xterm</code>, so ensure it is installed.</p> <p>Note: the desktop session is also sometimes known as the ARCADE software environment.</p>"},{"location":"publishing/#initialization-and-startup","title":"Initialization and Startup","text":""},{"location":"publishing/#running-container-process-owners","title":"Running container process owners","text":"<p>Containers in the CANFAR Science Platform are always executed as the CADC User and never as root. Operations that require root must be done at the build phase of the image.  If runtime root access is required, it can possibly be done by giving sudo access to specific actions.</p>"},{"location":"publishing/#session-container-initialization","title":"Session container initialization","text":"<p>Initialization for session containers is based on the session container type.  There are currently four types with different startup procedures: 1. <code>notebook</code>: it requires a <code>jupyter lab</code> executable 1. <code>carta</code>: initialization and startup is done through a customized script 1. <code>desktop-app</code>: desktop session startup is managed by the skaha infrastructure. 1. <code>contributed</code>: it will follow a customized startup script</p> <p>There may be multiple versions of the same type of session container, but the startup procedure for these must remain the same for them to be of the same type.</p>"},{"location":"publishing/#contributed-session-containers","title":"Contributed session containers","text":"<p>Contributed sessions are for custom-build, web-browser applications that are not officially created and maintained by CANFAR. The rules of building a container of type \"contributed\" on the CANFAR Science Platform are: 1. Incoming trafic will be over http (which may include websocket trafic) on port 5000 1. From the point of view of the container, requests will be received at the root path (/), but URLs in the browser will look like https:///, where  and  are subject to change. This path will initially be https://ws-uv.canfar.net/sessions/contrib/ 1. The instance will be started by a script in the image that must be available at /skaha/startup.sh and will be passed 1 parameter: the sessionid."},{"location":"publishing/#software-container-initialization","title":"Software container initialization","text":"<p>The <code>CMD</code> and <code>EXECUTABLE</code> directives in a CANFAR container <code>Dockerfile</code> will be ignored on startup.  Instead, bash within an xterm will run. <code>CMD</code> and <code>EXECUTABLE</code> are still useful for testing containers outside of CANFAR.</p> <p>If the container needs to do any runtime initialization, that can be done in a script named <code>init.sh</code> in the <code>/skaha</code> root directory.  This script must not block and needs to return control to the calling process.</p> <p>If <code>/skaha/init.sh</code> is provided, a sensible directive for testing the container via docker is <code>CMD [\"/skaha/init.sh\"]</code></p> <p>Another option is for containers to make available a file named <code>/skaha/startup.sh</code>.  If it exists, it will be called with a single parameter, which is the command <code>startup.sh</code> must run in order to execute on the platform.  So, the end of <code>startup.sh</code> should do: <code>exec \"$@\"</code> to execute the incoming parameter.  Containers should use startup.sh when environment must be made available to the context of the application.</p> <p></p>"},{"location":"publishing/#publishing-skaha-containers","title":"Publishing skaha containers","text":""},{"location":"publishing/#step-1-create-a-harbor-account","title":"Step 1: Create a harbor account","text":"<p>The CANFAR Science Platform hosts a private container registry. It is an instance of the Harbor open source project as a container registry. Session and software containers launched can be launched from this registry.</p> <p>If you have logged into harbor before then step 1 can be skipped.</p> <ol> <li>Go to https://images.canfar.net</li> <li>Press the <code>Login with OIDC Provider</code> button.</li> <li>Enter your CADC username and password.</li> <li>When prompter for a harbor userid, use your CADC username.</li> </ol> <p>After these steps you now have a harbor account and can see the project containers through its interface. If you wish to publish to any of the projects, contact the project admistrator (or contact support@canfar.net) and ask for 'Development' access to the project.</p>"},{"location":"publishing/#step-2-docker-login-to-harbor","title":"Step 2: Docker login to harbor","text":"<ol> <li>From the harbor portal, go to the top right, click on your username, then go to 'User Profile'.</li> <li>Set your CLI secret -- this is the password to use for <code>docker login</code> commands.  You can copy the existing, generated secret, or 'upload' (enter) your own.</li> <li>From the computer on which you have built the docker image you wish to publish, do a docker login:</li> </ol> <p><code>docker login images.canfar.net</code></p> <p>Your user is your CADC username, and your password the value of the CLI Secret mentioned above.</p>"},{"location":"publishing/#step-3-push-your-image-to-your-project","title":"Step 3: Push your image to your project","text":"<ol> <li>On the same computer, find the <code>IMAGE ID</code> of the image you'd like to push with the command</li> </ol> <p><code>docker images</code></p> <ol> <li>Tag the image for harbor:</li> </ol> <p><code>docker tag &lt;IMAGE ID&gt; images.canfar.net/&lt;PROJECT&gt;/&lt;MY IMAGE NAME&gt;:&lt;IMAGE VERSION&gt;</code> </p> <p>where:    * <code>&lt;PROJECT&gt;</code> is the project to which you've been granted Developer access.    * <code>&lt;MY IMAGE NAME&gt;</code> is the name of the image you are publishing.    * <code>&lt;IMAGE VERSION&gt;</code> is the version of the image.</p> <ol> <li>Push the image to harbor, with:</li> </ol> <p><code>docker push images.canfar.net/&lt;PROJECT&gt;/&lt;MY IMAGE NAME&gt;:&lt;IMAGE VERSION&gt;</code></p>"},{"location":"publishing/#step-4-label-your-image-type","title":"Step 4: Label your image type","text":"<ol> <li>Go back to https://images.canfar.net</li> <li>Click on your project, then on your newly pushed image (also called a repository in harbor).</li> <li>Select the 'artifact' with the correct version (tag).</li> <li>Under the 'Actions' drop-down, apply the approripate label to the artifact.</li> </ol>"},{"location":"publishing/#science-platform-actions","title":"Science Platform Actions","text":"<p>A number of the steps below can be done using the CANFAR Science Platform Portal at https://www.canfar.net</p>"},{"location":"publishing/#listing-images-on-canfar","title":"Listing images on CANFAR","text":"<p>Once publishing and labeling has been completed, the image will be visible.  It can then be seen on the Science Platform Portal, or with the folowing command:</p> <p><code>curl -E &lt;cadcproxy.pem&gt; https://ws-uv.canfar.net/skaha/v0/image</code></p>"},{"location":"publishing/#listing-resource-contexts","title":"Listing resource contexts","text":"<p>The available cores and RAM in skaha can be seen from the Science Platform, or viewed with:</p> <p><code>curl -E &lt;cadcproxy.pem&gt; https://ws-uv.canfar.net/skaha/v0/context</code></p> <p></p>"},{"location":"publishing/#launching-containers","title":"Launching containers","text":""},{"location":"publishing/#session-containers","title":"Session containers","text":"<ol> <li>Use the Science Platform Portal or this curl command to launch your newly published image:</li> </ol> <p><code>curl -E &lt;cadcproxy.pem&gt; https://ws-uv.canfar.net/skaha/v0/session -d \"name=&lt;arbitrary-name&gt;\" -d \"image=images.canfar.net/&lt;PROJECT&gt;/&lt;MY IMAGE NAME&gt;:&lt;IMAGE VERSION&gt;\"</code></p> <p>If non-default values for cores and/or ram is preferred, the parameters <code>-d cores=&lt;cores&gt;</code> and <code>-d ram=&lt;ram&gt;</code> can be added to the session launching command above.</p> <ol> <li>Use the Science Platform Portal or this curl command to find the URL to your session:</li> </ol> <p><code>curl -E &lt;cadcproxy.pem&gt; https://ws-uv.canfar.net/skaha/v0/session</code></p> <p>If this is the first time this image has been launched in may take a few minutes for the cloud do retrieve the image from harbor and send it to the selected node.</p>"},{"location":"publishing/#software-containers","title":"Software containers","text":"<p>It should just run as is. See also headless containers.</p>"},{"location":"publishing/#legacy-desktop-application-containers","title":"Legacy desktop application containers","text":"<p>Once a legacy desktop software container has been pushed to harbor, it must be labelled with <code>desktop-app</code>.</p> <p>To then make it appear in the <code>Applications-&gt;Astro Software</code> menu on the desktop a new desktop session must be started.</p> <p>The desktop menu items in <code>Applications-&gt;Astro Software</code> are organized by harbor project.  A sub-folder is created for each project.  Then, each version of the artifacts (images) within that project will be displayed in the project sub-folder.  For example, the desktop-app image identified by URI:</p> <p><code>images.canfar.net/skaha/terminal:1.0</code></p> <p>will be placed in the desktop menu like so:</p> <p><code>Applications -&gt; Astro Software -&gt; skaha -&gt; terminal:1.0</code></p> <p></p>"},{"location":"publishing/#testing","title":"Testing","text":"<p>Before publishing a new or modified image, testing should be done to ensure it works as expected.</p> <p>For session containers, nearly all testing can be done by using <code>docker</code> to run the image.  A port should be exposed so you can connect your browser to the locally running session.</p> <p>For legacy desktop application containers, docker will not be able to provide a graphical display of CASA windows, so most testing must be done in a skaha-desktop instance running in the cloud.</p> <p>The only requirement for a container is to ensure the web application to be launched with a <code>/skaha/startup.sh</code> script in the container, and the web application running on port 5000.</p> <p></p>"},{"location":"science-containers/","title":"CANFAR Science Platform User Containers","text":"<p>This repository (science-containers.git) includes build recipes for containers to be launched on the CANFAR science-platform.  Please feel free to contribute.</p>"},{"location":"science-containers/#canfar-maintained-containers","title":"CANFAR-maintained Containers","text":"<p>Curently the repository contains basic build system to build a hierarchy of default containers supported by CANFAR:</p> <pre><code>                        Ubuntu LTS\n                            |\n                    base (headless)\n                            |\n        _____________________ _ _ _ _ _ _ _\n        |                                  | \nastroml (headless)          Desktop Application (desktop-app)\n        |\nastroml-notebook (notebook) \nastroml-vscode   (contributed)\nastroml-desktop  (desktop-app)\n</code></pre> <ul> <li>The <code>base</code> is a <code>headless</code> container are built from a vanilla Ubuntu LTS, with extra operating system installed (compilers, development libraries), and conda install.</li> <li>The <code>astroml</code> is another <code>headless</code> container and is built with a large set of astronomy, machine learning, deep learning, visualisations and data science libraries. </li> <li>The <code>astroml-*</code> add visualisation and interactivity software. They can be launched as a <code>notebook</code> session, a <code>contributed</code> VSCode session, or a terminal through a <code>desktop</code> session.</li> <li>The <code>Desktop Application</code> containers do not typically derive from <code>astroml</code> and sometimes not from <code>base</code> containers, as they are legacy and may rely on now unsupported OS.</li> </ul> <p>There are CUDA-enabled versions of the containers, which include NVIDIA software and CUDA-powered libraries. They are built and named as <code>*-gpu</code>, i.e. <code>astroml-gpu</code>, <code>astroml-gpu-notebook</code>.</p>"},{"location":"science-containers/#canfar-user-customised-container","title":"CANFAR User Customised Container","text":"<p>If you want to build your own containers, documentation can be found in the docs directory. This directory also contains the building of the CANFAR usage documentation in the readthedocs style.</p>"},{"location":"science-containers/#todo","title":"TODO","text":""},{"location":"science-containers/#laundry-list-for-possible-actions","title":"Laundry list for possible actions","text":"<ul> <li>write a Makefile allowing: <code>make build astroml</code></li> <li>install jupyterlab extensions in home directory rather than /opt/conda</li> <li>configure jupyterlab containers to automatically creating directories  somewhere less intrusive than just <code>$HOME</code>:  <code>migrated astropy matplotlib yarn lab pip fontconfig serverconfig jedi numba</code></li> <li>launcher in jupyterlab has too many launching and confusing icons</li> <li>make a button on jupyterlab that will create a new environment automatically and create an icon</li> <li>build npm apps on container directories to avoid populating million of useless files</li> <li>environments in home directory with pip by default</li> <li>add other alternative notebooks such as querybook, cocalc, nteract_on_jupyter</li> </ul>"},{"location":"science-platform/","title":"CANFAR Science Platform Infrastructure","text":"<p>The repository for the platform service infrastructure and deployment configuration is at:  https://github.com/opencadc/science-platform</p> <p></p>"},{"location":"science-portal/","title":"CANFAR Science Portal","text":"<p>The CANFAR Science Portal runs at https://www.canfar.net It has the following services:</p> <ul> <li>Science Platform UI:  Science Platform UI</li> <li>Storage Management UI:  Storage Management UI (for both 'vault' and 'arc')</li> <li>Group Management UI:  Group Managment UI</li> <li>DOI Publication Service:  DOI Publication Service</li> </ul> <p></p>"}]}